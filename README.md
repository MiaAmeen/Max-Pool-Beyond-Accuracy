# Max-Pool-Beyond-Accuracy

Pruning is a popular technique for reducing the computational cost of convolutional neural networks (CNNs), however it is less explored that how it affects networks with various pooling strategies. Since max pooling naturally suppresses weaker activations, we suggest that it makes CNNs' robust to pruning, thereby masking pruning-induced harm. In order to assess any advantages pooling methods may have after pruning, we methodically compare max pooling and average pooling under structured and unstructured pruning. AlexNet and VGG experiments on CIFAR-10 and CIFAR-100 will examine whether pooling and pruning interact to affect the efficiency and recoverability of the model.

Our final report can be accessed here:[The Impact of Pooling Methods on the Sparsity-Accuracy Tradeoff Curve in Pruned Networks](https://www.overleaf.com/read/cbbrscnhsbzt#624317)
